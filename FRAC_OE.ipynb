{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import jit , njit\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import log2\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import timeit\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.cluster import KMeans\n",
    "!pip install scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note** that you might need to uncomment some portions of balance and fairness error methods while executing Bank dataset (three valued protected group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Config Variables\n",
    "n0 = 1000  # number of p=0 points in metric space\n",
    "\n",
    "K = 10 # No of clusters\n",
    "A= 5 # No of attributes\n",
    "iterations = 1  # maximum iteration in clustering\n",
    "runs =10\n",
    "option='Kmeans'  #Kmedian\n",
    "dataset ='Adult'  #Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "def load_Bank(data_dir=''):\n",
    "\n",
    "    data_dir = data_dir\n",
    "    _path = 'bank-full_p_6col.csv'\n",
    "    data_path = os.path.join(data_dir, _path)\n",
    "    df = pandas.read_csv(data_path, sep=',')\n",
    "    \n",
    "    #print(df.head())\n",
    "    #print(len(df))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "def load_Adult(data_dir=''):\n",
    "\n",
    "    data_dir = data_dir\n",
    "    _path = 'adult_p.csv'\n",
    "    data_path = os.path.join(data_dir, _path)\n",
    "\n",
    "\n",
    "    df = pandas.read_csv(data_path, sep=',')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "load_Adult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='Bank':\n",
    "    df=load_Bank()\n",
    "    df= df.round(decimals=5)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    print(len(df))\n",
    "    df['type'] = df['type']-1\n",
    "    typ = df['type'].values\n",
    "    #print(len(typ))\n",
    "    #print(df.head(10))\n",
    "    c3 = np.count_nonzero(typ == 2)\n",
    "\n",
    "    print(c1/(c1+c2+c3))\n",
    "    print(c2/(c1+c2+c3))\n",
    "    print(c3/(c1+c2+c3))\n",
    "\n",
    "\n",
    "    print(c1)\n",
    "    print(c2)\n",
    "    print(c3)\n",
    "    dfDropped = df.drop(columns=['type'])\n",
    "\n",
    "elif dataset='Adult':\n",
    "    df=load_Adult()\n",
    "    df= df.round(decimals=5)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    print(len(df))\n",
    "    #df['type'] = df['type']-1\n",
    "    typ = df['gender'].values\n",
    "    #print(len(typ))\n",
    "    #print(df.head(10))\n",
    "    c1 = np.count_nonzero(typ == 0)\n",
    "    c2 = np.count_nonzero(typ == 1)\n",
    "\n",
    "    print(c1/(c1+c2))\n",
    "    print(c2/(c1+c2))\n",
    "\n",
    "    print(c1)\n",
    "    print(c2)\n",
    "    dfDropped = df.drop(columns=['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VanillaKmeans(X, k,seedValue):\n",
    "    \"\"\"\n",
    "    specify the number of clusters k and\n",
    "    the maximum iteration to run the algorithm\n",
    "    \"\"\"\n",
    "    n_row, n_col = X.shape\n",
    "    maxiter=100\n",
    "    \n",
    "    # randomly choose k data points as initial centroids\n",
    "   \n",
    "    \n",
    "    rand_indices = np.random.choice(n_row, size = k)\n",
    "    centroids = X[rand_indices]\n",
    "    cost_variation=[]\n",
    "    cnt =0\n",
    "    for itr in range(maxiter):\n",
    "        # compute distances between each data point and the set of centroids\n",
    "        # and assign each data point to the closest centroid\n",
    "        distances_to_centroids = pairwise_distances(X, centroids, metric = 'sqeuclidean',force_all_finite=True)\n",
    "        cluster_assignment = np.argmin(distances_to_centroids, axis = 1)\n",
    "\n",
    "        # select all data points that belong to cluster i and compute\n",
    "        # the mean of these data points (each feature individually)\n",
    "        # this will be our new cluster centroids\n",
    "        new_centroids=[]\n",
    "        for i  in range(k):\n",
    "            #print(i)\n",
    "            assign= np.array([X[cluster_assignment == i]])\n",
    "          \n",
    "            med = np.mean(assign[0],axis=0)\n",
    "            new_centroids.append(med)             \n",
    "        \n",
    "        \n",
    "        # if the updated centroid is still the same,\n",
    "        # then the algorithm converged\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        #print(\"Median\"+str(new_centroids))\n",
    "            \n",
    "        #print(\"Centrer\"+str(centroids))\n",
    "        if np.all(centroids == new_centroids):\n",
    "            cnt = cnt+1\n",
    "            if cnt ==4:\n",
    "                break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "        \n",
    "        heterogeneity = 0\n",
    "        for i in range(k):\n",
    "            # note that pairwise_distance only accepts 2d-array\n",
    "            cluster_data = X[cluster_assignment == i]\n",
    "            distances = pairwise_distances(cluster_data, [centroids[i]], metric = 'euclidean')\n",
    "            heterogeneity += np.sum(distances ** 2) #sq euclidean \n",
    "        cost_variation.append(heterogeneity)\n",
    "        \n",
    "    \n",
    "    return centroids, cluster_assignment, heterogeneity,cost_variation#(cost)\n",
    "\n",
    "\n",
    "def VanillaKmedian(X, k,seedValue):\n",
    "    \"\"\"\n",
    "    specify the number of clusters k and\n",
    "    the maximum iteration to run the algorithm\n",
    "    \"\"\"\n",
    "    n_row, n_col = X.shape\n",
    "    maxiter=100\n",
    "    \n",
    "    # randomly choose k data points as initial centroids\n",
    "    #if seed is not None:\n",
    "     #   np.random.seed(seed)\n",
    "    \n",
    "    rand_indices = np.random.choice(n_row, size = k)\n",
    "    centroids = X[rand_indices]\n",
    "    cost_variation=[]\n",
    "    cnt =0\n",
    "    for itr in range(maxiter):\n",
    "        # compute distances between each data point and the set of centroids\n",
    "        # and assign each data point to the closest centroid\n",
    "        distances_to_centroids = pairwise_distances(X, centroids, metric = 'sqeuclidean',force_all_finite=True)\n",
    "        cluster_assignment = np.argmin(distances_to_centroids, axis = 1)\n",
    "\n",
    "        # select all data points that belong to cluster i and compute\n",
    "        # the mean of these data points (each feature individually)\n",
    "        # this will be our new cluster centroids\n",
    "        new_centroids=[]\n",
    "        for i  in range(k):\n",
    "            #print(i)\n",
    "            assign= np.array([X[cluster_assignment == i]])\n",
    "            med = np.median(assign[0],axis=0)\n",
    "            new_centroids.append(med)             \n",
    "       \n",
    "        # if the updated centroid is still the same,\n",
    "        # then the algorithm converged\n",
    "        new_centroids = np.array(new_centroids)\n",
    "       \n",
    "        if np.all(centroids == new_centroids):\n",
    "    \n",
    "                break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "        \n",
    "        heterogeneity = 0\n",
    "        for i in range(k):\n",
    "            # note that pairwise_distance only accepts 2d-array\n",
    "            cluster_data = X[cluster_assignment == i]\n",
    "            distances = pairwise_distances(cluster_data, [centroids[i]], metric = 'euclidean')\n",
    "            heterogeneity += np.sum(distances ** 2) #sq euclidean \n",
    "        cost_variation.append(heterogeneity)\n",
    "        \n",
    "    \n",
    "    return centroids, cluster_assignment, heterogeneity,cost_variation#(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_print(f,*args,**kwargs):\n",
    "    #print(*args,**kwargs)\n",
    "    print(*args,**kwargs,file=f)\n",
    "\n",
    "def load_dataset(csv_name):\n",
    "    # read the dataset from csv_name and return as pandas dataframe\n",
    "    df = pd.read_csv(csv_name, header=None)\n",
    "    return df\n",
    "\n",
    "\n",
    "def k_random_index(df,K):\n",
    "    # return k random indexes in range of dataframe\n",
    "    return random.sample(range(0, len(df)), K)\n",
    "\n",
    "def find_k_initial_centroid_Kmeans(df,K,seedValue):\n",
    "    kmeans=KMeans(n_clusters=int(K),random_state=int(seedValue)).fit(df)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "          \n",
    "def find_k_initial_centroid(df,K):\n",
    "    centroids = []    # make of form [ [x1,y1]....]\n",
    "\n",
    "    rnd_idx = k_random_index(df,K)\n",
    "    \n",
    "    for i in rnd_idx:\n",
    "        coordinates =[]\n",
    "        for a in range(0,A):\n",
    "            coordinates.append(df.loc[i][a])\n",
    "        centroids.append(coordinates)   #df is X,Y,....., Type\n",
    "\n",
    "    return centroids\n",
    "\n",
    "#nOt using\n",
    "def calc_distance(x1, y1, x2, y2):\n",
    "    # returns the euclidean distance between two points\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "def calc_distance_a(centroid, point):\n",
    "    #print('çalculating distance\\n')\n",
    "\n",
    "    sum_ = 0\n",
    "\n",
    "    for i in range(0, len(centroid)):\n",
    "        sum_ = sum_ + (centroid[i]-point[i])**2\n",
    "\n",
    "    return sum_ #**0.5\n",
    "\n",
    "@njit(parallel=False)\n",
    "def find_distances_fast(k_centroids, df):\n",
    "    #print(\"Inside fast distances\")\n",
    "\n",
    "   \n",
    "    dist = np.zeros((len(k_centroids),len(df),A+2),np.float64)\n",
    "    Kcnt = 0 \n",
    "    for c in k_centroids:  #K-centroid is of form [ c1=[x1,y1.....z1], c2=[x2,y2....z2].....]\n",
    "        #print(\"C: \"+str(Kcnt))\n",
    "        l = np.zeros((len(df),A+2),np.float64)\n",
    "        \n",
    "       # for row in df:\n",
    "        index = 0 \n",
    "        for row in df:                # row is now x,y,z......type\n",
    "            # append all coordinates to point\n",
    "            dis = np.sum((c- row[:A])**2)#calc_distance_a(c, point)\n",
    "            #Processing the vector for list\n",
    "            row_list = np.array([dis])\n",
    "            #append distance or l norm\n",
    "            row_list = np.append(row_list,row[:A+1])\n",
    "            #append all coordinates #append type of this row\n",
    "  \n",
    "            l[index] = row_list\n",
    "            index = index + 1\n",
    "            #l.append([calc_distance(c[0], c[1], row[0], row[1]), row[0], row[1], row[2]])  # [dist, X, Y,....Z , type]\n",
    "            # l contains list of type [dist,X,Y.....,Z,type] for each points in metric space\n",
    "        dist[Kcnt]= l\n",
    "        Kcnt = Kcnt + 1\n",
    "\n",
    "    # return dist which contains distances of all points from every centroid\n",
    "\n",
    "    return dist\n",
    "\n",
    "def find_distances(k_centroids, df):\n",
    "    dist = []\n",
    "    for c in k_centroids:  #K-centroid is of form [ c1=[x1,y1.....z1], c2=[x2,y2....z2].....]\n",
    "        l = []\n",
    "      \n",
    "\n",
    "        for index, row in df.iterrows():                # row is now x,y,z......type\n",
    "            point =[]\n",
    "            for a in range(0, A):\n",
    "                point.append(row.iloc[a])  # append all coordinates\n",
    "\n",
    "            dis = calc_distance_a(c, point)\n",
    "            #Processing the vector for list\n",
    "            row_list = [dis]\n",
    "            #append distance or l norm\n",
    "\n",
    "            for a in range(0, A):\n",
    "                row_list.append(row.iloc[a])   #append all coordinates\n",
    "            #print(row.iloc[a+1])\n",
    "            row_list.append(row.iloc[a+1])   #append type of this row\n",
    "\n",
    "            l.append(row_list)\n",
    "            #l.append([calc_distance(c[0], c[1], row[0], row[1]), row[0], row[1], row[2]])  # [dist, X, Y,....Z , type]\n",
    "            # l contains list of type [dist,X,Y.....,Z,type] for each points in metric space\n",
    "        dist.append(l)\n",
    "\n",
    "    # return dist which contains distances of all points from every centroid\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def sort_and_valuation(dist):\n",
    "    sorted_val = []\n",
    "  \n",
    "    for each_centroid_list in dist:\n",
    "        each_centroid_list_sorted = sorted(each_centroid_list, key=lambda x: (x[A+1], x[0]))  # A+1 is index of type , 0 is dist\n",
    "        sorted_val.append(each_centroid_list_sorted)\n",
    "\n",
    "        # sort on basis of type & then dist.\n",
    "        # Now all whites are towards start and all black are after white as they have additional V added to their valuation\n",
    "        # Among the whites, the most closest is at start of list as it has more valuation.\n",
    "        # Similarly sort the black points among them based on distance as did with white\n",
    "\n",
    "    return sorted_val\n",
    "\n",
    "\n",
    "def clustering(sorted_valuation, hashmap_points,K):\n",
    "    n = len(hashmap_points.keys())  # total number of points in metric space\n",
    "    \n",
    "    \n",
    "    cluster_assign = []\n",
    "\n",
    "    for i in range(0, K):\n",
    "        cluster_assign.append([])  # initially all clusters are empty\n",
    "  \n",
    "    map_index_cluster = []\n",
    "    for i in range(0,K+2):\n",
    "        map_index_cluster.append(0)\n",
    "        #initially check all sorted evaluation from 0th index \n",
    "    \n",
    "    number_of_point_alloc = 0\n",
    "    curr_cluster = 0\n",
    "    \n",
    "    # until all points are allocated\n",
    "    while number_of_point_alloc != n:  # As convergence is guaranteed that all points will be allocated to some cluster set\n",
    "        #print('Number of point alloc : '+str(number_of_point_alloc))\n",
    "        start_inde = map_index_cluster[curr_cluster % K]\n",
    "        \n",
    "        for inde in range(start_inde,len(sorted_valuation[curr_cluster % K])):\n",
    "            each = sorted_valuation[curr_cluster % K][inde]\n",
    "            # each is (dist,X,Y,....Z,type)\n",
    "          \n",
    "            \n",
    "            if hashmap_points[tuple(each[1: -1])] == 0:    # each is (dist, X,Y,....Z, type)\n",
    "                cluster_assign[curr_cluster].append(each)\n",
    "                hashmap_points[tuple(each[1: -1])] = 1\n",
    "                number_of_point_alloc += 1\n",
    "                map_index_cluster[curr_cluster % K] = inde  #next time start from here as isse prev all allocated\n",
    "                break\n",
    "\n",
    "        curr_cluster = (curr_cluster + 1) % K\n",
    "\n",
    "    return cluster_assign\n",
    "\n",
    "\n",
    "def update_centroids_median(cluster_assign,K):\n",
    "    new_centroids = []\n",
    "    for k in range(0, K):\n",
    "      \n",
    "        cAk =  np.array(cluster_assign[k])\n",
    "        cAk = np.delete(cAk,[0,-1],axis=1)\n",
    "        #print(len(cAk))\n",
    "        if len(cAk) %2 ==0 and len(cAk)>0: \n",
    "            cc = [np.median(np.array(cAk[:-1])[:,cl]) for cl in range(0,cAk.shape[1])]\n",
    "            new_centroids.append(cc)\n",
    "        elif len(cAk) %2 !=0 and len(cAk)>0:\n",
    "            cc = [np.median(np.array(cAk)[:,cl]) for cl in range(0,cAk.shape[1])]\n",
    "            new_centroids.append(cc)\n",
    "        elif len(cAk)==0:\n",
    "            print(\"Error: No centroid found updation error\")\n",
    "  \n",
    "    return new_centroids\n",
    "        \n",
    "\n",
    "\n",
    "def update_centroids(cluster_assign,K):\n",
    "\n",
    "    new_centroids = []\n",
    "    for k in range(0, K):\n",
    "\n",
    "        sum_a = []\n",
    "\n",
    "        for i in range(0, A):\n",
    "            sum_a.append(0)\n",
    "\n",
    "        for each in cluster_assign[k]:\n",
    "            sum_a = [sum(x) for x in zip(sum_a, each[1:-1])]\n",
    "            #each is (dist,X,Y,.....Z,type)\n",
    "\n",
    "        new_coordinates = []\n",
    "        for a in range(0, A):\n",
    "            new_coordinates.append(sum_a[a] / len(cluster_assign[k]))\n",
    "        new_centroids.append(new_coordinates)\n",
    "        k=k+1\n",
    "\n",
    "\n",
    "\n",
    "    return new_centroids\n",
    "\n",
    "\n",
    "def calc_clustering_objective(k_centroid, cluster_assign,K):\n",
    "    cost = 0\n",
    "\n",
    "    for k in range(0, K):\n",
    "\n",
    "        for each in cluster_assign[k]:  #each is (dist, X,Y,....,Z,type)\n",
    "            dd = calc_distance_a(k_centroid[k], each[1:-1])\n",
    "            cost = cost + (dd)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "#needs updation if dataset is Bank\n",
    "def calc_fairness_error(df, cluster_assign,K):\n",
    "    U = []  # distribution of each type in original target dataset for each J = 0 , 1....\n",
    "    P_k_sum_over_j = []  # distribution in kth cluster  sum_k( sum_j(   Uj * j wale/total_in_cluster ) )\n",
    "\n",
    "    f_error = 0\n",
    "    cnt_j_0 = 0\n",
    "    cnt_j_1 = 0\n",
    "  #  cnt_j_2 = 0\n",
    "    cnt = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row.iloc[-1] == 1:\n",
    "            cnt_j_1 += 1\n",
    "        elif row.iloc[-1] == 0:\n",
    "            cnt_j_0 += 1\n",
    "      #  elif row.iloc[-1] == 2:\n",
    "        #    cnt_j_2 += 1\n",
    "            \n",
    "        cnt += 1\n",
    "\n",
    "    U.append(cnt_j_0 / cnt)\n",
    "    U.append(cnt_j_1 / cnt)\n",
    "    #U.append(cnt_j_2 / cnt)\n",
    "\n",
    "    \n",
    "\n",
    "    for k in range(0, K):  # for each cluster\n",
    "\n",
    "        for j in range(0, len(U)):   #for each demographic group\n",
    "\n",
    "            cnt_j_cluster = 0\n",
    "            cnt_total = 0\n",
    "\n",
    "            for each in cluster_assign[k]:\n",
    "                if int(each[-1]) == j:    #each is (dist,X, Y.....,Z,type)\n",
    "                    cnt_j_cluster += 1\n",
    "                cnt_total += 1\n",
    "                \n",
    "            if cnt_j_cluster !=0 and cnt_total != 0:\n",
    "                P_k_sum_over_j.append(-U[j] * np.log((cnt_j_cluster / cnt_total)/U[j]))\n",
    "            else:\n",
    "                P_k_sum_over_j.append(0)  #log(0)=0 considered\n",
    "\n",
    "    for each in P_k_sum_over_j:\n",
    "        f_error += each\n",
    "\n",
    "    return f_error\n",
    "\n",
    "#Balance function needs updation for Bank Dataset by uncommenting the commented part\n",
    "def calc_balance(cluster_assign,K):\n",
    "    S_k = []  # balance of each k cluster\n",
    "    balance = 0  # min (S_k)\n",
    "\n",
    "    for k in range(0, K):\n",
    "        cnt_j_0 = 0\n",
    "        cnt_j_1 = 0\n",
    "       # cnt_j_2 = 0\n",
    "        cnt = 0\n",
    "        for each in cluster_assign[k]:\n",
    "\n",
    "            if int(each[-1]) == 1:\n",
    "                cnt_j_1 += 1\n",
    "            elif int(each[-1]) == 0:\n",
    "                cnt_j_0 += 1\n",
    "           # elif int(each[-1]) == 2:\n",
    "           #     cnt_j_2 += 1\n",
    "                \n",
    "            cnt += 1\n",
    "\n",
    "        if cnt_j_0 != 0 and cnt_j_1 != 0 :#and cnt_j_2!= 0:\n",
    "            S_k.append(min([cnt_j_0 / cnt_j_1, cnt_j_1 / cnt_j_0 ]))#, cnt_j_1 / cnt_j_2 , cnt_j_2 / cnt_j_1 , cnt_j_0 / cnt_j_2, cnt_j_2 / cnt_j_0 ]))\n",
    "        elif cnt_j_0 == 0 or cnt_j_1 ==0  :#or cnt_j_2==0:\n",
    "            S_k.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    balance = min(S_k)\n",
    "\n",
    "    return balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Step1 : Load the dataset\n",
    "            \n",
    "    list_fair_K=[]\n",
    "    list_obj_K =[]       \n",
    "    list_balance_K=[]\n",
    "    list_obj_K_std =[]\n",
    "    list_fair_K_std=[]\n",
    "    list_balance_K_std=[]\n",
    "        \n",
    "    \n",
    "    os.makedirs('Adult_kmeans_center')\n",
    "    \n",
    "    for kk in [2,5,10,15,20,30,40]:\n",
    "        K = kk\n",
    "        \n",
    "        print(\" K==\"+str(K)+\"  \")\n",
    "        \n",
    "        list_fair_run=[]\n",
    "        cost_variation_ck_collec =[]\n",
    "        cost_variation_collec = []\n",
    "        list_obj_run =[]       \n",
    "        list_balance_run=[]\n",
    "        seeds = [0,100,200,300,400,500,600,700,800,900,1000,1100]\n",
    "      \n",
    "        for run in range(0,runs):\n",
    "            np.random.seed(seeds[run])\n",
    "            random.seed(seeds[run])\n",
    "            f = open('Adult_kmeans_center/K_'+str(K)+'_run_'+str(run)+'_output.txt', 'a')\n",
    "        \n",
    "\n",
    "            list_fair_iter=[]\n",
    "            list_obj_iter =[]\n",
    "            list_balance_iter=[]\n",
    "\n",
    "            # Step2 : Find initial K random centroids using k_random_index(df) & find_k_initial_centroid(df)\n",
    "            if option=='Kmeans':\n",
    "                k_centroid,_,_,cost_variation= VanillaKmeans(dfDropped.values,kk,seeds[run])#VanillaKmeans()find_k_initial_centroid_Kmeans(dfDropped,kk,seeds[run])#find_k_initial_centroid(df,kk)\n",
    "            else:\n",
    "                k_centroid,_,_,cost_variation= VanillaKmedian(dfDropped.values,kk,seeds[run])#VanillaKmeans()find_k_initial_centroid_Kmeans(dfDropped,kk,seeds[run])#find_k_initial_centroid(df,kk)\n",
    "            cost_variation_ck = []\n",
    "            \n",
    "            for eac in cost_variation:\n",
    "                cost_variation_ck.append(eac)\n",
    "            \n",
    "           \n",
    "            prev_assignment =[]\n",
    "            cluster_assignment = []\n",
    "\n",
    "            for i in range(0, K):\n",
    "                cluster_assignment.append([])  # initially all clusters are empty\n",
    "\n",
    "            sum_time = 0\n",
    "            curr_itr = 0\n",
    "            prev_objective_cost=-1\n",
    "            objective_cost = 0\n",
    "                # Step3 : Find distances from the centroids using find_distances() with list of [ [x1,y1,z1..] , [x2,y2,z2..]....] centroids format list\n",
    "            while curr_itr < iterations:# and prev_objective_cost != objective_cost:\n",
    "\n",
    "                \n",
    "             \n",
    "                df1 = df.values\n",
    "                k_centroids1= np.array(k_centroid)\n",
    "                \n",
    "                dist = find_distances_fast(k_centroids1, df1)\n",
    "             \n",
    "                valuation = sort_and_valuation(dist)\n",
    "               \n",
    "                #Step5 : Perform clustering using valuation matrix & hashmap of all points in metric\n",
    "                hash_map = {}\n",
    "                for index, row in df.iterrows():\n",
    "                    temp = tuple(row[:-1])\n",
    "                    hash_map.update({tuple(row[:-1]): 0})   #dict is of form { (x,y): 0 , ....}\n",
    "            \n",
    "                prev_assignment = cluster_assignment\n",
    "                cluster_assignment = clustering(valuation, hash_map,K)\n",
    "\n",
    "                \n",
    "                balance = calc_balance(cluster_assignment,K)\n",
    "              \n",
    "                f_error = calc_fairness_error(df, cluster_assignment,K)\n",
    "               \n",
    "                clustering_cost = calc_clustering_objective(k_centroid,cluster_assignment,K)\n",
    "                if curr_itr!=0:\n",
    "                     prev_objective_cost = objective_cost\n",
    "                    \n",
    "                objective_cost = np.round(clustering_cost,3)\n",
    "                \n",
    "                cost_variation_ck.append(objective_cost)\n",
    "                \n",
    "\n",
    "                list_balance_iter.append(str(balance))\n",
    "                list_obj_iter.append(str(objective_cost))\n",
    "                list_fair_iter.append(str(f_error))\n",
    "\n",
    "            \n",
    "\n",
    "                #Step7 : Find new centroids using mean of all points in current assignment\n",
    "                if option=='Kmeans':\n",
    "                    k_centroid_temp = update_centroids(cluster_assignment,K)\n",
    "                else:\n",
    "                    k_centroid_temp = update_centroids_median(cluster_assignment,K)\n",
    "                \n",
    "                clustering_cost_temp = calc_clustering_objective(k_centroid_temp,cluster_assignment,K)\n",
    "                cost_variation.append(np.round(clustering_cost_temp,3))\n",
    "               \n",
    "               \n",
    "                curr_itr += 1\n",
    "\n",
    "                dual_print(f,'-----------------------------Finished-----------------------------------------------\\n')\n",
    "\n",
    "\n",
    "              \n",
    "           \n",
    "            #Step 10 : Find balance , fairness error , and clustering objective or cost\n",
    "\n",
    "            balance_converged = calc_balance(cluster_assignment,K)\n",
    "            f_error_converged = calc_fairness_error(df, cluster_assignment,K)\n",
    "            clustering_cost_converged = calc_clustering_objective(k_centroid,cluster_assignment,K)\n",
    "\n",
    "            \n",
    "            cost_variation_collec.append(cost_variation)\n",
    "          \n",
    "            \n",
    "            f.close()\n",
    "            run  = run +1\n",
    "            list_obj_run.append(clustering_cost_converged)\n",
    "            list_fair_run.append(f_error_converged)\n",
    "            list_balance_run.append(balance_converged)\n",
    "        \n",
    "        \n",
    "        print(\"Cost variations over run\\n\")\n",
    "        print(str(cost_variation_collec))\n",
    "        print(\"\\nCost variations over run- CK\\n\")\n",
    "        print(str(cost_variation_ck_collec))\n",
    "        print(\"\\nbalance variations over run\")\n",
    "        print(str(list_balance_run))\n",
    "        print(\"\\nfairness error over run\")\n",
    "        print(str(list_fair_run))\n",
    "        print(\"#\"*30)\n",
    "        \n",
    "        \n",
    "        list_obj_K.append(np.mean(np.array(list_obj_run)))\n",
    "        list_fair_K.append(np.mean(np.array(list_fair_run)))\n",
    "        list_balance_K.append(np.mean(np.array(list_balance_run)))\n",
    "        list_obj_K_std.append(np.std(np.array(list_obj_run)))\n",
    "        list_fair_K_std.append(np.std(np.array(list_fair_run)))\n",
    "        list_balance_K_std.append(np.std(np.array(list_balance_run)))\n",
    "        \n",
    "        \n",
    "    print('Mean Cost variation over K ',list_obj_K)\n",
    "    print('Mean Fairness Error variation over K ',list_fair_K)\n",
    "    print('Mean Balance variation over K ',list_balance_K)\n",
    "    print('Std Cost variation over K ',list_obj_K_std)\n",
    "    print('Std F.Error variation over K ',list_fair_K_std)\n",
    "    print('Std Balance variation over K ',list_balance_K_std)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
